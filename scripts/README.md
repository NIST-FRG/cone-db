# Cone Calorimeter Scripts

This folder contains scripts and explorers to parse, prepare, and process experimental data across various sources of Cone Calorimeter tests within the Cone Database (within the Material Flammability Database)

Required: Python 3.11 or higher

## Installing packages
Run the following command to install the required packages:
```bash
pip install -r requirements.txt
```

## Raw Experimental Data
**Legacy Cone Calorimeter Data** : 
- Raw PDF and LlamaParsed Markdown files found in [LlamaTime Shared Google Drive](https://drive.google.com/drive/u/0/folders/0AKNwSqPWrWJYUk9PVA) in the “Scanned Files” folder
- Ultimatley, these files will live on the firedata repo, within a subfolder of Cone data labeled "Box".
- **<ins> For now: Pull the folder "Unsorted" from the LLamaTime Shared drive and place into **cone-db/data/raw/Box**. Please create this folder if it doesn’t exist. </ins>**
**FTT Data**:
- Raw csv files generated by the FTT Cone are stored in the on the firedata server (\\firedata\FLAMMABILITY_DATA\DATA\Cone).



# Running Scripts
**<ins> Clone this cone-db repository.</ins>**
Firstly, navigate the terminal to **/cone-db/scripts/**

## Markdown Sorting
**Legacy Cone Calorimeter Data Only** : 
- In the future, this will be performed on the firedata server and will not need to be run by individuals

Run: 
```
python DetectFormat_Cone.py
```
- Markdown files within the Unsorted Folder are seperated into formats A, B, and C based upon what data/metadata is availble
- Currently, these would be stored in the folders **cone-db/data/raw/Box/md_A B or C** which are autogenerated

## Pre-parsing
**Legacy Cone Calorimeter Data Only** : 
- In the future, this will be performed on the firedata server and will not need to be run by individuals

Run:
```
python Preparse_Cone-mdA.py
python Preparse_Cone-mdB.py
python Preparse_Cone-mdC.py

```
- Input: LlamaParsed markdown files at **../data/raw/Box/md_A B or C**
- These scripts parse through the llamaparsed markdown files of a specific format, seperate the files into individual tests, and extract the test data and metadata from each.
- Data Table columns are compiled as a singular dataframe and exported as a csv file.
    - Performs restrictive parsing, removing unnecessary whitespace, dashes, stars, etc  
    - Output Directory : **\cone-db\data\pre-parsed\md_A\test####_pdf_name.csv**
    - Data Columns can vary significantly within and between formats, but generally the following columns are present:
        - Format A: | Time (s) | HRRPUA (kW/m²) | THRPUA (MJ/m²) | MLRPUA (g/s·m²) | Mass Loss (kg/m²) | HT Comb (MJ/kg) | Extinction Area (m²/kg) | CO₂ (kg/kg) | CO (kg/kg) | H₂O (kg/kg) | H'carbs (kg/kg) | HCl (kg/kg) |
        - Format B: | Time (s) | HRRPUA (kW/m²) | THRPUA (MJ/m²) | MLR (g/s)) | Mass (g) | HT Comb (MJ/kg) | Extinction Area (m²/kg) | CO₂ (kg/kg) | CO (kg/kg) | H₂O (kg/kg) | H'carbs (kg/kg) | HCl (kg/kg) | MFR (kg/s) | V Duct (m3/s) | Soot (kg/kg) | Total Smoke (m2/kg) |
        - Format C: | Time (s) | HRRPUA (kW/m²) | THRPUA (MJ/m²) | MLR (g/s)) | HT Comb (MJ/kg) | Air/Sample (kg/kg) | CO₂ (kg/kg) | CO (kg/kg) | 

- Metadata contains information on testing conditions and parameters, material data, additional measured values, etc.  
    - Metadata is initialized to match the fields in FTT and other data, so for many tests a large portion of the metadata attributes may be left blank.
    - For now, all metadata from the markdown files is also appeneded to the metadata item comments. There are several cases where a metadata attribute is ignored or missassigned, the comments section keeps a full record of the available information.
    
- Output: Both the preparsed data csv file and json metadata file are stored in **../data/preparsed/Box/md_A B or C**. Ultimatley these will be on firedata.
 

## Parsing

Run:
```
python Parse_Cone-mdA.py
python Parse_Cone-mdB.py
python Parse_Cone-mdC.py
python Parse_Cone-FTT.py

`still in development`
python Parse_Cone-FAA.py
python Parse_Cone-MIDAS.py
python Parse_Cone-Netzch.py
```
- Parsing scripts walk through their respective preparsed legacy data or raw device files on firedata in order to generate uniform csv data and json metadata files which will go to the explorer for further processing and analysis. Parsed csv files will all have the following 9 data columns from which key parameters and datasets can be calculate. 

| Time (s) | Mass (g) | HRR (kW) | MFR (kg/s) | T Duct (K) | O2 (Vol %) | CO2 (Vol %) | CO (Vol %) | K Smoke (1/m) |
|----------|----------|----------|------------|------------|------------|-------------|------------|---------------|

- For most of the legacy datasets, some or all of these columns may not be populated based on what information was provided. In these cases, bonus columns (ex: HRRPUA (kW/m2), MLR (g/s), CO2 (kg/kg), "Extinction Area (m2/kg), etc...) are appended for maximum information retrieval.
- Parsed csv files are stored within device specific subfolders of **Exp-Data_Parsed** 

- Metadata files are generated for the apparatus specific parsing scripts, while legacy metadata files are passed from the preparsed folder. These metadata files are stored within subfolders **Metadata/Parsed**
    - Because of the preparation steps needed to process legacy Cone data, each test has two metadata files. Thus, unlike the other apparatus in the Material Flammability Database, Cone metadata is seperated into **Parsed** (preSmURF) and **Prepared-Final** (postSmURF).



## Prepare/Systematic User Review of Files (SmURF)
Preparation of Cone data mainly consists of the Systematic User Review of Files (SmURF). This process entails the careful review of both data and metadata generated by parsing and preparsing scripts for each legacy test. The need for SmURFing is twofold. First, the process of converting pdfs to markdowns and preparsing to csv and json files often can create a number of errors. Students will remediate these errors as needed to ensure data and metadata accuracy prior to publication. Second, the majority of legacy Cone data lacks the proper naming conventions to be encorporated into the Material Flammability Database. Mainly, these tests lack the unique material-ID upon which the database is structured. As part of the SmURF process, students will link legacy tests to published reports, generating a material-ID of the format `<material_name>-<report_identifier>` to ensure data of that test series is processed together. Following the SmURF preparation process, test data and metadata are exported to the 'Prepared-Final' stage, where they can be autoprocessed and encoporated into the larger database.


<ins>Navigate the terminal to /cone-db/scripts/Cone_Explorer</ins>

Run:
```
python -m streamlit run Cone_Main.py
```
Further information on the SmURF process is found in the Cone Explorer README.


## Autoprocess

<ins>Navigate the terminal to \cone-db\scripts.</ins>

Run:
```
python Autoprocess_Cone.py
```

- The autoprocessing script walks through the **Exp-Data_Prepared-Final** data folders, seperating tests by test series (MatID_HF_Orient). For each series, the script calls Autoprocess_Cone_IndSeries.py to calculate additional key features and thermophysical properties of the data. The script also performs statistical analysis and outlier detection for these features amongst the test series. Listed below is the current collection of programmatically calculated values as “Property/Value (unit) [metadata_field_name]”:
    - Average Mass Loss Rate (g/s-m2) **[MLRPUA]**
    - Residue Yield (g/g) **[mf/m0_g/g]**
    - Peak HRR (kW/m2) **[peak_q_dot_kw/m2]**

Full List of Additional Calculated Features : https://docs.google.com/document/d/1uqdBjGeKTKFXSLZ6MUrCiqjBiaWAMfYQEK9GYorT1Aw/edit?tab=t.0

